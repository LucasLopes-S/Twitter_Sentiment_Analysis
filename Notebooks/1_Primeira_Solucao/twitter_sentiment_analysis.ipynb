{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter_sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yg2aIQsO2YzF",
        "tkGQV9qYjJ1w",
        "nvDEwqpdjRoj",
        "1FG_mkBCjV88",
        "A5HEmnJzjo7U",
        "T-tiCTXdjsxh",
        "RYQIhO3VSIZr",
        "tCfzDnmi-ctV",
        "FmHpud8m-kp9",
        "D4jn4sqRSRU7",
        "aRgglqXaWhFh",
        "ZQXEvzvUZGJn",
        "bu4BE6MAabej",
        "NZsv3XrMjx5J",
        "m7I6G75Rj219",
        "ODahEnITj6hn",
        "IOb9dWLEkCi6",
        "7ONuinVokIDF"
      ],
      "authorship_tag": "ABX9TyPqN/0vpeIVbQeg9JDSpUuZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JefteLG/Twitter_Sentiment_Analysis/blob/main/Notebooks/1_Primeira_Solucao/twitter_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8P3MLXMdRTQ"
      },
      "source": [
        "<center><h1><strong>Twitter Sentiment Analysis<strong><h1></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duqiar06fBqQ"
      },
      "source": [
        "O objetivo desse estudo de caso √© criar um modelo que analisa um ou mais Tweets para prever o sentimento(Positivo ou Negativo) presente em cada Tweet. √â usado o Processamento de linguagem natural(NLP) juntamente com aprendizagem de maquina para construir esse modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg2aIQsO2YzF"
      },
      "source": [
        "# Testes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag4flFQjzhtb"
      },
      "source": [
        "luska = 'quero @user remover o @user agora'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lqDk7duzoWa"
      },
      "source": [
        "newtext = [word for word in luska.split()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_BrfGxaK5T9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdw4YWIr4GNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b872023-1a35-46d0-d2f6-8c6254a5a34c"
      },
      "source": [
        "#Baixar os dados\n",
        "!wget --no-check-certificate \\\n",
        "    https://raw.githubusercontent.com/JefteLG/Twitter_Sentiment_Analysis/main/Data_Set/twitter.csv \\\n",
        "    -O /tmp/twitter.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-22 22:37:09--  https://raw.githubusercontent.com/JefteLG/Twitter_Sentiment_Analysis/main/Data_Set/twitter.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3103165 (3.0M) [text/plain]\n",
            "Saving to: ‚Äò/tmp/twitter.csv‚Äô\n",
            "\n",
            "/tmp/twitter.csv    100%[===================>]   2.96M  12.7MB/s    in 0.2s    \n",
            "\n",
            "2021-01-22 22:37:09 (12.7 MB/s) - ‚Äò/tmp/twitter.csv‚Äô saved [3103165/3103165]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRGF4vKf4W6s"
      },
      "source": [
        "tweets_df = pd.read_csv('/tmp/twitter.csv')\n",
        "tweets_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EL9iOzd42sZ"
      },
      "source": [
        "tweet = tweets_df[tweets_df['id']==4]['tweet'].iloc[0]\n",
        "tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVDDI6GJfsSy"
      },
      "source": [
        "!pip install demoji\n",
        "import demoji as em\n",
        "\n",
        "em.download_codes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWwlotCJg4se"
      },
      "source": [
        "text = 'Ol√° üòÄ'\n",
        "\n",
        "em.findall(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXNbx7fJj0kY"
      },
      "source": [
        "from pprint import pprint\n",
        "seq = \"I bet you didn't know that üòÄ, üôã‚Äç‚ôÇÔ∏è, and üôã‚Äç‚ôÄÔ∏è are three different emojis.\"\n",
        "pprint(seq.encode('unicode-escape'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LrqN-0beT29"
      },
      "source": [
        "**TESTE2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3RxzwmoeSDj",
        "outputId": "265d4eec-797e-4b84-9b1e-ca8f4b18a974"
      },
      "source": [
        "import re\r\n",
        "import string\r\n",
        "\r\n",
        "text1=\"O Zyoo n√£o √© o 'melhor' do mundo\"\r\n",
        "text2='Eu n√£o gosto de \"jogar\" futebol'\r\n",
        "\r\n",
        "limpar_aspas_simples=\"'\"\r\n",
        "limpar_aspas_duplas='\"'\r\n",
        "\r\n",
        "textfim1 = re.sub(limpar_aspas_simples,' ', text1)\r\n",
        "textfim2 = re.sub(limpar_aspas_duplas,' ', text2)\r\n",
        "\r\n",
        "print(textfim1)\r\n",
        "print(textfim2)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O Zyoo n√£o √© o  melhor  do mundo\n",
            "Eu n√£o gosto de  jogar  futebol\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSWhiOLv60Az"
      },
      "source": [
        "**TESTE3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOqd4d5R63QG",
        "outputId": "07f8c2e0-ee9e-40bf-bb2e-14650e6df3b5"
      },
      "source": [
        "compare_list = \"FB needs to hurry up and add a laugh/cry button üò¨üò≠üòìü§¢üôÑüò± Since eating my feelings has not fixed the world's problems.\"\r\n",
        "\r\n",
        "from nltk.tokenize import RegexpTokenizer\r\n",
        "match_tokenizer = RegexpTokenizer(\"[\\w']+\")\r\n",
        "# match_tokens = [sent for sent in compare_list]\r\n",
        "match_tokens = []\r\n",
        "\r\n",
        "for sent in compare_list:   \r\n",
        "    match_tokens.append(match_tokenizer.tokenize(sent))\r\n",
        "\r\n",
        "print(match_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['F'], ['B'], [], ['n'], ['e'], ['e'], ['d'], ['s'], [], ['t'], ['o'], [], ['h'], ['u'], ['r'], ['r'], ['y'], [], ['u'], ['p'], [], ['a'], ['n'], ['d'], [], ['a'], ['d'], ['d'], [], ['a'], [], ['l'], ['a'], ['u'], ['g'], ['h'], [], ['c'], ['r'], ['y'], [], ['b'], ['u'], ['t'], ['t'], ['o'], ['n'], [], [], [], [], [], [], [], [], ['S'], ['i'], ['n'], ['c'], ['e'], [], ['e'], ['a'], ['t'], ['i'], ['n'], ['g'], [], ['m'], ['y'], [], ['f'], ['e'], ['e'], ['l'], ['i'], ['n'], ['g'], ['s'], [], ['h'], ['a'], ['s'], [], ['n'], ['o'], ['t'], [], ['f'], ['i'], ['x'], ['e'], ['d'], [], ['t'], ['h'], ['e'], [], ['w'], ['o'], ['r'], ['l'], ['d'], [\"'\"], ['s'], [], ['p'], ['r'], ['o'], ['b'], ['l'], ['e'], ['m'], ['s'], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2OXSA5xiPzo"
      },
      "source": [
        "# Estrutura do Projeto\n",
        "O projeto pr√°tico de An√°lise de sentimento do Twitter √© dividido nas seguintes tarefas:\n",
        "\n",
        "- Tarefa n¬∫ 1: Entender a Declara√ß√£o do Problema e o caso de neg√≥cios.\n",
        "- Tarefa n¬∫ 2: Importar bibliotecas e conjuntos de dados.\n",
        "- Tarefa n¬∫ 3: Executar a an√°lise explorat√≥ria de dados.\n",
        "- Tarefa n¬∫ 4: Plotar a nuvem de palavras.\n",
        "- Tarefa n¬∫ 5: Executar a limpeza de dados - remover pontua√ß√£o.\n",
        "- Tarefa n¬∫ 6: Executar a limpeza de dados - remover palavras de parada(stop words).\n",
        "- Tarefa n¬∫ 7: Executar vetoriza√ß√£o de contagem (Tokenization).\n",
        "- Tarefa n¬∫ 8: Criar um pipeline para remover palavras irrelevantes, pontua√ß√£o e realizar tokeniza√ß√£o.\n",
        "- Tarefa n¬∫ 9: Treinar um Classificador Naive Bayes.\n",
        "- Tarefa n¬∫ 10: Avaliar o desempenho do modelo treinado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkGQV9qYjJ1w"
      },
      "source": [
        "## Tarefa n¬∫ 1: Entender a Declara√ß√£o do Problema e o caso de neg√≥cios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzAaeCtFkbGS"
      },
      "source": [
        "- Nesse notebook √© usado o Processamento de linguagem natural(NLP) juntamente com aprendizagem de maquina para construir um modelo que analisa milhares de Tweets para prever o sentimentos das pessoas.\n",
        "\n",
        "- A Inteligencia artificial e a analise de sentimentos baseadas em aprendizado de maquina √© crucial para empresas, visto que, os insight revelado pela analise visa indicar o grau de qualidade dos servi√ßos e/ou produtos da empresa de acordo com os clientes.(Trabalhos Futuros)\n",
        "\n",
        "- Esse projeto √© diretamente aplicav√©l a praticamente qualquer empresa que disponhe de meios onlines(Twitter, Instagran, Facebook, WebSite) para interagir com seus clientes.(Trabalhos Futuros)\n",
        "\n",
        "- Os algoritmos podem ser usados para detectar e possivelmente sinalizar autamaticamente tweets de odio e racismo.(Trabalhos Futuros)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvDEwqpdjRoj"
      },
      "source": [
        "## Tarefa n¬∫ 2: Importar bibliotecas e conjuntos de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iWAJGZ-nSc9"
      },
      "source": [
        "#Pacotes essenciais para analise numericas, manipula√ß√£o de data frames e visualiza√ß√£o de dados.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULlYu-EV-JHS"
      },
      "source": [
        "# Pacote para a cria√ß√£o de nuuvens de palavras na tarefa n¬∫ 4\n",
        "!pip install WordCloud\n",
        "from wordcloud import WordCloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQm_dk5noTY1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d82d8d7-d0bf-4e72-927d-22bcc3a714cb"
      },
      "source": [
        "# A base de dados desse exemplo est√° no meu repositorio do github e pode ser baixada no link abaixo.\n",
        "!wget --no-check-certificate \\\n",
        "    https://raw.githubusercontent.com/JefteLG/Twitter_Sentiment_Analysis/main/Data_Set/data_base_estudos/twitter.csv \\\n",
        "    -O /tmp/twitter.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-02 12:58:04--  https://raw.githubusercontent.com/JefteLG/Twitter_Sentiment_Analysis/main/Data_Set/data_base_estudos/twitter.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3103165 (3.0M) [text/plain]\n",
            "Saving to: ‚Äò/tmp/twitter.csv‚Äô\n",
            "\n",
            "/tmp/twitter.csv    100%[===================>]   2.96M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-02-02 12:58:05 (23.7 MB/s) - ‚Äò/tmp/twitter.csv‚Äô saved [3103165/3103165]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgzrtLjdsqFT"
      },
      "source": [
        "# Utilizando o pandas para ler o arquivo CSV e estrutura-lo em um data frame na variavel `tweets_df` por meio do metodo read_csv().\n",
        "tweets_df = pd.read_csv('/tmp/twitter.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmnIKKR_tDjr"
      },
      "source": [
        "# visualiza√ß√£o1.\n",
        "tweets_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goEV6Sck7LY_"
      },
      "source": [
        "# visualiza√ß√£o2.\n",
        "tweets_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dNaBl0Tu2v6"
      },
      "source": [
        "# Informa√ß√µes sobre a quantidade de Tweets, memoria, tipo de dados e dados faltantes.\n",
        "tweets_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1fkzOu8u3dQ"
      },
      "source": [
        "# Exclus√£o da coluna `id`, n√£o √© um dado relevante para o modelo.\n",
        "# tweets_df = tweets_df.drop(columns=['id'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FG_mkBCjV88"
      },
      "source": [
        "## Tarefa n¬∫ 3: Executar a an√°lise explorat√≥ria de dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIrEPJ-N_qdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "d5a86719-359f-47c7-d2aa-7529d9b2c9ba"
      },
      "source": [
        "# Esta √© uma fun√ß√£o no n√≠vel dos eixos e desenhar√° o mapa de calor para os eixos ativos no momento. Nesse caso a fun√ß√£o verifica se existe dados faltantes.\n",
        "sns.heatmap(tweets_df.isnull(), yticklabels=False, cbar=False, cmap=\"Blues\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3fdbd2e208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ydidIVtLsok"
      },
      "source": [
        "# O m√©todo sns.countplot() √© usado para mostrar as contagens de observa√ß√µes em cada categoria categ√≥rica usando barras(Histograma).\n",
        "sns.countplot(data=tweets_df, x='label', palette='Set2')\n",
        "plt.show()\n",
        "\n",
        "# sns.countplot(tweets_df['label'], label='Count', palette='Set2')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODWALt2K91Ma"
      },
      "source": [
        "# Nova coluna com o tamanho dos tweets\n",
        "tweets_df['length'] = tweets_df['tweet'].apply(len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGCIFqFdFVD-"
      },
      "source": [
        "**Teste**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gI6KT9gZFg60"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWscYirZGwDl"
      },
      "source": [
        "tweets_df[tweets_df['id']==31958]['tweet'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSB_7j5VGPUJ"
      },
      "source": [
        "tweetTK = TweetTokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPeDf5MXpY-U"
      },
      "source": [
        "tweetTK.tokenize(tweets_df[tweets_df['id']==31958]['tweet'].iloc[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKFlUz0sFaxS"
      },
      "source": [
        "**Fim Teste**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI_41ZMN-TaI"
      },
      "source": [
        "# Descobrir o tamanho maximo, minimo e medio dos tweets.\n",
        "tweets_df['length'].plot(bins=100, kind='hist', figsize=(12,8), color='g')\n",
        "\n",
        "tweets_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_KTzfnzadgh"
      },
      "source": [
        "# Selecionar a menor frase\n",
        "tweets_df[tweets_df['length']==11]['tweet'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UqITftfiNIZ"
      },
      "source": [
        "# Separa o DataFrame em dois Dataframes, um com sentimentos positivos e o outro com sentimentos negativos.\n",
        "positive_df = tweets_df[tweets_df['label']==0]\n",
        "negative_df = tweets_df[tweets_df['label']==1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kppOxtAiu_Q"
      },
      "source": [
        "positive_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keY3soLSiweY"
      },
      "source": [
        "negative_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5HEmnJzjo7U"
      },
      "source": [
        "## Tarefa n¬∫ 4: Plotar a nuvem de palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz3DsDvrxwN1"
      },
      "source": [
        "# Criar uma lista de tweets\n",
        "sentences = tweets_df['tweet'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLDny0ox3CqD"
      },
      "source": [
        "# O m√©todo join() pega todos os itens em um iter√°vel e os une em uma string. √© usado o espa√ßo como separador\n",
        "sentences_as_one_string = \" \".join(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIm2zkE68BTi"
      },
      "source": [
        "# Nuvem de palavras de todos os tweets\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(WordCloud().generate(sentences_as_one_string))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBKJtXYnCiIV"
      },
      "source": [
        "# Nuvem de palavras de todos os tweets NEGATIVOS\n",
        "negative_sentences = negative_df['tweet'].tolist()\n",
        "negative_as_one_string = \" \".join(negative_sentences)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(WordCloud().generate(negative_as_one_string))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A54mv1tmD32T"
      },
      "source": [
        "# Nuvem de palavras de todos os tweets POSITIVOS\n",
        "positive_sentences = positive_df['tweet'].tolist()\n",
        "positive_as_one_string = \" \".join(positive_sentences)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(WordCloud().generate(positive_as_one_string))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-tiCTXdjsxh"
      },
      "source": [
        "## Tarefa n¬∫ 5: Executar a limpeza de dados - remover pontua√ß√£o"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cRytRI8O0HK"
      },
      "source": [
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uFn5yCbPOrL"
      },
      "source": [
        "text = 'Good morning beautiful people :)... I am having fun learning Machine learning and artificial intelligence'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxK8GFjgbJ0G"
      },
      "source": [
        "[Artigo sobre a remo√ß√£o de pontua√ß√µes de uma string](https://towardsdatascience.com/how-to-efficiently-remove-punctuations-from-a-string-899ad4a059fb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYQIhO3VSIZr"
      },
      "source": [
        "### Metodo #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCfzDnmi-ctV"
      },
      "source": [
        "#### Metodo #1.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVxmUCskQBxm"
      },
      "source": [
        "test_punc_remove = [char for char in text if char not in string.punctuation]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXSRQNuzQZ_2"
      },
      "source": [
        "test_punc_remove"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Q5OH_AWqKp"
      },
      "source": [
        "test_punc_remove = ''.join(test_punc_remove)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdMeAyQZXF8S"
      },
      "source": [
        "test_punc_remove"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmHpud8m-kp9"
      },
      "source": [
        "#### Metodo #1.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnfRhlkG-oKb"
      },
      "source": [
        "text_remove_punct = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bElj4lx7-oDK"
      },
      "source": [
        "for char in text:\n",
        "  if char not in string.punctuation:\n",
        "    text_remove_punct.append(char)\n",
        "\n",
        "new_text = ''.join(text_remove_punct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMdjOxEC-n7E"
      },
      "source": [
        "new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4jn4sqRSRU7"
      },
      "source": [
        "### Metodo #2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPGscUDASXic"
      },
      "source": [
        "punct = string.punctuation + string.digits  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3EBTeJkTTYT"
      },
      "source": [
        "table_tst = str.maketrans('','',punct)\n",
        "newtext = text.translate(table_tst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhv6s43CWH0D"
      },
      "source": [
        "newtext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRgglqXaWhFh"
      },
      "source": [
        "### Metodo #3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6er7mq7ZBgi"
      },
      "source": [
        "punct = string.punctuation + string.digits  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMaSjYuEXO9_"
      },
      "source": [
        "table_ = str.maketrans(punct, ' '*len(punct))\n",
        "newtext = ' '.join(text.translate(table_).split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs3RJV2sZD43"
      },
      "source": [
        "newtext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQXEvzvUZGJn"
      },
      "source": [
        "### Metodo #4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ1tH_mfZIsN"
      },
      "source": [
        "punct = string.punctuation + string.digits  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsMhHzNvZLcS"
      },
      "source": [
        "for s in punct:\n",
        "  text = text.replace(s, '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY7mlEIUZLUR"
      },
      "source": [
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu4BE6MAabej"
      },
      "source": [
        "### Metodo #5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9ElzdhlaeIg"
      },
      "source": [
        "import re\n",
        "newtext = re.sub(r'[^A-Za-z]+', ' ', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9vhABfQadsI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fc7c11ed-6112-438c-e564-323074af7b87"
      },
      "source": [
        "newtext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Good morning beautiful people I am having fun learning Machine learning and artificial intelligence'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZsv3XrMjx5J"
      },
      "source": [
        "## Tarefa n¬∫ 6: Executar a limpeza de dados - remover palavras de parada(stop words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw6Pxu__gPkh"
      },
      "source": [
        "# Pacotes para facilitar o processamento de linguagem natural\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG4VEtkrg1UA"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCxdrCa8QgWt"
      },
      "source": [
        "text = 'Good morning beautiful people :)... I am having fun learning Machine learning and artificial intelligence in 2020'\n",
        "\n",
        "# caracteres para remover\n",
        "puncts = string.digits + string.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KO6IGRjRBYH"
      },
      "source": [
        "# remo√ß√£o de caracteres\n",
        "text_remove_punct = [char for char in text if char not in puncts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtxMfbn-TIkl"
      },
      "source": [
        "# pega todos os itens em um iter√°vel e os une em uma string.\n",
        "text_remove_punct = ''.join(text_remove_punct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU0hltVZT3Vg"
      },
      "source": [
        "text_remove_punct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra6faW2EQqqd"
      },
      "source": [
        "# remo√ß√£o de stopword\n",
        "text_remove_stopword = [word for word in text_remove_punct.split() if word.lower() not in stopwords.words('english')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2kFMXv6Uyex"
      },
      "source": [
        "text_remove_stopword "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RjLXTptfZLr"
      },
      "source": [
        "**Pipeline remover pontua√ßoes e stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV-wOtpsfjpY"
      },
      "source": [
        "def pipeline(text):\r\n",
        "  # frase sem nenhuum tratamento\r\n",
        "  print(text)\r\n",
        "  # remover as pontua√ßoes\r\n",
        "  text_remove_punct = [char for char in text if char not in puncts]\r\n",
        "  # unir todos os elemntos da lista\r\n",
        "  text_remove_punct = ''.join(text_remove_punct)\r\n",
        "  # frase com pontua√ß√µes removidas\r\n",
        "  print(text_remove_punct) \r\n",
        "\r\n",
        "  # remover as stopword\r\n",
        "  text_remove_stopword = [word for word in text_remove_punct.split() if word.lower() not in stopwords.words('english')]\r\n",
        "  text_remove_stopword = ' '.join(text_remove_stopword)\r\n",
        "  # frase com stopwords removidas\r\n",
        "  print(text_remove_stopword)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2S9ALjrhHo2"
      },
      "source": [
        "pipeline(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7I6G75Rj219"
      },
      "source": [
        "## Tarefa n¬∫ 7: Executar vetoriza√ß√£o de contagem(Tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYk95p3VPFlm"
      },
      "source": [
        "Aprofundar:\n",
        "\n",
        "[Feature Extraction from Text (USING PYTHON)](https://www.youtube.com/watch?v=7YacOe4XwhY&ab_channel=MachineLearningTV)\n",
        "\n",
        "[Feature Extraction in Scikit Learn](https://www.youtube.com/watch?v=y_X4hXjTFNQ&ab_channel=DataTalks)\n",
        "\n",
        "[NLP - Text Preprocessing and Text Classification (using Python)](https://www.youtube.com/watch?v=nxhCyeRR75Q&ab_channel=MachineLearningTV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgcotjlfsFBi"
      },
      "source": [
        "**Metodologia: Bag of Word**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozUCJAzaY9hW"
      },
      "source": [
        "#Classe responsavel por Converter uma cole√ß√£o de documentos de texto em uma matriz de contagens de tokens\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flJY0smjG7G3"
      },
      "source": [
        "sample_data=[\n",
        "             'Porcaria de produto',\n",
        "             'Obrigado pelo retorno estou muito satisfeito com seu trabalho?',\n",
        "             'Voces ainda v√£o me responder?',\n",
        "             'O pior servi√ßo de atendimento de todos!!!'\n",
        "             ]\n",
        "\n",
        "sample_data2=[\n",
        "              'Hello World',\n",
        "              'Hello Hello hello World',\n",
        "              'Hello World world world'\n",
        "              ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDwLv6l_H8tM"
      },
      "source": [
        "# Aprender o vocabulario do texto e retornar um array, [n_samples, n_features]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(sample_data)\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qyf_aQuuQo5"
      },
      "source": [
        "# versao tokenizada das strings no dataset\r\n",
        "# (id palavra, id frase)  frequencia da palavra na frase \r\n",
        "c = X\r\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scJDbm9MV-N-",
        "outputId": "8aef7dca-5840-41ef-d7a2-f0a9e5122801"
      },
      "source": [
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'porcaria': 11, 'de': 4, 'produto': 12, 'barato': 2, 'ruim': 15, 'obrigado': 8, 'pelo': 9, 'retorno': 14, 'estou': 5, 'muito': 7, 'satisfeito': 16, 'com': 3, 'seu': 18, 'trabalho': 20, 'voces': 21, 'ainda': 0, 'v√£o': 22, 'me': 6, 'responder': 13, 'pior': 10, 'servi√ßo': 17, 'atendimento': 1, 'todos': 19}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmm7uZfTIGol"
      },
      "source": [
        "#Retorne uma representa√ß√£o densa desta matriz.\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODahEnITj6hn"
      },
      "source": [
        "## Tarefa n¬∫ 8: Criar um pipeline para remover palavras irrelevantes, pontua√ß√£o e realizar tokeniza√ß√£o"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b7pb76fioM5"
      },
      "source": [
        "import pandas as pd\n",
        "import string as st\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f0gdT_cf0T8"
      },
      "source": [
        "# Baixar os dados\n",
        "\n",
        "# data set\n",
        "!wget --no-check-certificate \\\n",
        "  https://raw.githubusercontent.com/JefteLG/Twitter_Sentiment_Analysis/main/Data_Set/data_base_estudos/twitter.csv \\\n",
        "  -O /tmp/twitter.csv\n",
        "\n",
        "# stop word\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmwIfBnciOfw"
      },
      "source": [
        "# ler os dados\n",
        "msg_tweet_df = pd.read_csv('/tmp/twitter.csv')\n",
        "\n",
        "# remover digitos e pontua√ß√µes\n",
        "# char_remove = st.digits + st.punctuation\n",
        "char_remove = st.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12tw94bFZxkf"
      },
      "source": [
        "# limpar as menssagens\n",
        "def message_cleaning(msg):\n",
        "\n",
        "  # remo√ß√£o das pontua√ß√µes\n",
        "  text_remove_punct = [char for char in msg if char not in char_remove]\n",
        "  text_remove_punct = ''.join(text_remove_punct)\n",
        "\n",
        "  # remo√ß√£o das palavras de parada\n",
        "  text_remove_stopword = [word for word in text_remove_punct.split() if word.lower() not in stopwords.words('english')]\n",
        "  # text_remove_stopword = ' '.join(text_remove_stopword)\n",
        "  return text_remove_stopword"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vStSLQ4ElUQJ"
      },
      "source": [
        "# aplica a fun√ß√£o para limpar as mensagens para cada tweet da serie\r\n",
        "tweet = msg_tweet_df['tweet'].apply(message_cleaning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR5s0hllWOEr"
      },
      "source": [
        "type(tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T7c606Jmpzq"
      },
      "source": [
        "tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogTu3dRVybiY"
      },
      "source": [
        "# tweet especifico pr√© limpeza\r\n",
        "print(tweet[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNnr23ib0TOY"
      },
      "source": [
        "# tweet especifico p√≥s limpeza\r\n",
        "print(msg_tweet_df['tweet'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkFjEIYY1dTS"
      },
      "source": [
        "#Realiza a Tokeniza√ß√£o dos tweets utilizando a limpeza de mensagens primeiro\n",
        "vectorizer = CountVectorizer(analyzer=message_cleaning, dtype='uint8')\n",
        "tweets_countvectorizer = vectorizer.fit_transform(msg_tweet_df['tweet'])\n",
        "# tweets_countvectorizer = CountVectorizer(analyzer=message_cleaning, dtype='uint8').fit_transform(msg_tweet_df['tweet']).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6eWgAi79Hg_"
      },
      "source": [
        "# po = tweets_countvectorizer.toarray()\r\n",
        "po = tweets_countvectorizer\r\n",
        "print(po)\r\n",
        "# print(po[0][33370:33380])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_9AWqon1UgF"
      },
      "source": [
        "# informa√ß√µes sobre a tokrniza√ß√£o dos tweets \r\n",
        "type(tweets_countvectorizer)\r\n",
        "tweets_countvectorizer.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejaRBayj5k32"
      },
      "source": [
        "print(vectorizer.get_feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8V8jDlj5stZ"
      },
      "source": [
        "print(tweets_countvectorizer.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl8nudHRjYF_"
      },
      "source": [
        "t =tweets_countvectorizer.toarray()\r\n",
        "print(t[0][14500:14600])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0lI6lr01DkI"
      },
      "source": [
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYb1VWIaqzQ8"
      },
      "source": [
        "# conjunto de tweets vetorizados em um array de presen√ßa e frequencia de palavras em uma frase\r\n",
        "X = tweets_countvectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHIh440bqy8b"
      },
      "source": [
        "# conjunto de rotulos binarios para classificar a emo√ß√£o(positivo, negetiva) para cada elemento do meu conjunto X\r\n",
        "y = msg_tweet_df['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBt-Uk-LlxfV"
      },
      "source": [
        "**TESTE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btUXHFqCl1H1"
      },
      "source": [
        "# limpar as menssagens\r\n",
        "def message_cleaning(msg):\r\n",
        "\r\n",
        "  # remo√ß√£o das pontua√ß√µes\r\n",
        "  text_remove_punct = [char for char in msg if char not in char_remove]\r\n",
        "  text_remove_punct = ''.join(text_remove_punct)\r\n",
        "\r\n",
        "  # remo√ß√£o das palavras de parada\r\n",
        "  text_remove_stopword = [word for word in text_remove_punct.split() if word.lower() not in stopwords.words('english')]\r\n",
        "  text_remove_stopword = ' '.join(text_remove_stopword)\r\n",
        "  return text_remove_stopword\r\n",
        "\r\n",
        "# aplica a fun√ß√£o para limpar as mensagens para cada tweet da serie\r\n",
        "tweet2 = msg_tweet_df['tweet'].apply(message_cleaning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PqCZcsVnJua"
      },
      "source": [
        "#Realiza a Tokeniza√ß√£o dos tweets utilizando a limpeza de mensagens primeiro\r\n",
        "vectorizer = CountVectorizer(dtype='uint8')\r\n",
        "tweets_countvectorizer = vectorizer.fit_transform(tweet2)\r\n",
        "# tweets_countvectorizer = CountVectorizer(analyzer=message_cleaning, dtype='uint8').fit_transform(msg_tweet_df['tweet']).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTaKJ1nPmanu"
      },
      "source": [
        "tweets_countvectorizer.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOb9dWLEkCi6"
      },
      "source": [
        "## Tarefa n¬∫ 9: Treinar um Classificador Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TILiPLqyt6xr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d768c84b-11fb-4199-fd66-447cf2c1385d"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31962, 47386)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lInlkb-Qz8sz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a47869-db41-4459-f977-f71d51288be7"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31962,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGB3eRrPjuqJ"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1G3m9ze5Hzk"
      },
      "source": [
        "# Separa√ß√£o dos dados de treino e teste\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbFNi3XT6tEw"
      },
      "source": [
        "# print(X_train)\r\n",
        "ht = X_train.toarray()\r\n",
        "print(ht[25568][41680])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jngxhDl0-2rz"
      },
      "source": [
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm3KI8kEj3Qw"
      },
      "source": [
        "print(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xj41yy2j3F6"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8pCb9iAj26M"
      },
      "source": [
        "print(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYWJ7ARx5v_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff02dd70-dba2-40d3-dccf-90c4c0148f13"
      },
      "source": [
        "# Utiliza√ß√£o do algoritmo de Naive Bayes para o treinamento\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "\r\n",
        "NB_classifier = MultinomialNB()\r\n",
        "NB_classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ONuinVokIDF"
      },
      "source": [
        "## Tarefa n¬∫ 10: Avaliar o desempenho do modelo treinado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng8oUUUnnfBo"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\r\n",
        "import seaborn as sns "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv8V_OGXnyQd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e7802fd-7964-4b65-f12c-dc656d93ac6b"
      },
      "source": [
        "y_predict_test = NB_classifier.predict(X_test)\r\n",
        "cm = confusion_matrix(y_test, y_predict_test)\r\n",
        "cm\r\n",
        "# sns.heatmap(cm, annot=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5773,  185],\n",
              "       [ 198,  237]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObWst89KqSXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19eed09a-1417-4476-8a32-2e01dac4da2b"
      },
      "source": [
        "print(classification_report(y_test, y_predict_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97      5958\n",
            "           1       0.56      0.54      0.55       435\n",
            "\n",
            "    accuracy                           0.94      6393\n",
            "   macro avg       0.76      0.76      0.76      6393\n",
            "weighted avg       0.94      0.94      0.94      6393\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}